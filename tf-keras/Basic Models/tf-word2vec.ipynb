{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXQESE1Np3nM"
      },
      "source": [
        "# Word2Vec (Word Embedding)\n",
        "\n",
        "Implement Word2Vec algorithm to compute vector representation of words, with Tensorflow 2.0. This example is using a small chunk of Wikipedia to train from.\n",
        "\n",
        "More info: Mikolov, Tomas et al. \"Efficient Estimation of Word Representations in Vector Space.\", 2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO62urt9qgRd",
        "outputId": "df732b21-2dd2-4ddc-dd43-0c5775218a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220929150707)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (22.9.24)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.49.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjGwgB4cqJJt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import urllib\n",
        "import zipfile\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpsUXJJhqm-F"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "# Training Parameters\n",
        "learning_rate: float = 0.1\n",
        "batch_size: int = 128\n",
        "num_steps: int = 3000000\n",
        "display_step: int = 10000\n",
        "eval_step: int = 200000\n",
        "\n",
        "# Evaluate Parameters\n",
        "eval_words: List[str] = [\"five\", \"of\", \"going\", \"hardware\", \"american\", \"britain\"]\n",
        "\n",
        "# Word2Vec Parameters\n",
        "embedding_size: int = 200 # Dimension of the embedding vector\n",
        "max_vocabulary_size: int = 50000 # Total number of different words in the vocabulary\n",
        "min_occurence: int = 10 # Remove all words that does not appear at least n times\n",
        "skip_window: int = 3 # How many times to consider left and right\n",
        "num_skips: int = 2 # How many times to reuse an input to generate a label\n",
        "num_sampled: int = 64 # Number of negative example to sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2ecqVhrtB2",
        "outputId": "ff2e1fbf-3c35-4c66-9c43-900546f748b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading the dataset... (It may take some time)\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Download a small chunk of Wikipedia articles collection.\n",
        "url: str = 'http://mattmahoney.net/dc/text8.zip'\n",
        "data_path = \"text8.zip\"\n",
        "\n",
        "test_words: str\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "  print(\"Downloading the dataset... (It may take some time)\")\n",
        "  filename, _ = urllib.request.urlretrieve(url, data_path)\n",
        "  print(\"Done!\")\n",
        "\n",
        "# Unzip the dataset file. Text has already been processed\n",
        "with zipfile.ZipFile(data_path) as f:\n",
        "  text_words = f.read(f.namelist()[0]).lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRscIOxQsu5e",
        "outputId": "4ccfc488-ec26-4d51-ca60-eed1be527046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not a byte\n",
            "Words count: 17005207\n",
            "Unique words: 253854\n",
            "Vocabulary size: 47135\n",
            "Most common words: [('UNK', 444176), (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764), (b'in', 372201), (b'a', 325873), (b'to', 316376), (b'zero', 264975), (b'nine', 250430)]\n"
          ]
        }
      ],
      "source": [
        "# Build the dictionary and replace rare words with UNK token\n",
        "count = [(\"UNK\", -1)]\n",
        "# Retrieve the most common words\n",
        "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
        "# Remove samples with less than 'min_occurence' occurences\n",
        "for i in range(len(count) -1, -1, -1):\n",
        "  if count[i][1] < min_occurence:\n",
        "    count.pop(i)\n",
        "  else:\n",
        "    # Retrieve a word if, or assign it index 0 (\"UNK\") if not in dictionary\n",
        "    break\n",
        "# Compute the vocabulary size.\n",
        "vocabulary_size = len(count)\n",
        "# Assign an id to each word\n",
        "word2id = dict()\n",
        "for i, (word, _) in enumerate(count):\n",
        "  try:\n",
        "    if len(word.decode(\"utf8\")) > 1:\n",
        "      word2id[word.decode(\"utf8\")] = i\n",
        "  except Exception as e:\n",
        "    print(\"Not a byte\")\n",
        "  else:\n",
        "    word2id[word] = i\n",
        "\n",
        "data = list()\n",
        "unk_count = 0\n",
        "\n",
        "for word in text_words:\n",
        "  # Retrieve a word id or assign it index 0 ('UNK') if not in dictionary\n",
        "  index = word2id.get(word, 0)\n",
        "  if index == 0:\n",
        "    unk_count = unk_count + 1\n",
        "  data.append(index)\n",
        "\n",
        "count[0] = (\"UNK\", unk_count)\n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
        "\n",
        "print(\"Words count:\", len(text_words))\n",
        "print(\"Unique words:\", len(set(text_words)))\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "print(\"Most common words:\", count[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUbMMpEhu_Dm"
      },
      "outputs": [],
      "source": [
        "data_index: int = 0\n",
        "# Generate training batch for the skip-gram model\n",
        "def next_batch(batch_size: int, num_skips: int, skip_window: int):\n",
        "  global data_index\n",
        "  assert batch_size % num_skips == 0\n",
        "  assert num_skips <= 2 * skip_window\n",
        "\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "  # get window size (words left and right and current one)\n",
        "  span = 2 * skip_window + 1\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  if data_index + span > len(data):\n",
        "    data_index = 0\n",
        "  buffer.extend(data[data_index:data_index + span])\n",
        "  data_index = data_index + span\n",
        "\n",
        "  for i in range(batch_size // num_skips):\n",
        "    context_words = [w for w in range(span) if w != skip_window]\n",
        "    words_to_use = random.sample(context_words, num_skips)\n",
        "    for j, context_word in enumerate(words_to_use):\n",
        "      batch[i * num_skips + j] = buffer[skip_window]\n",
        "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
        "\n",
        "      if data_index == len(data):\n",
        "        buffer.extend(data[0:span])\n",
        "        data_index = span\n",
        "      else:\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = data_index + 1\n",
        "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "  data_index = (data_index + len(data) - span) % len(data)\n",
        "  return batch, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4NVbWk0xalV"
      },
      "outputs": [],
      "source": [
        "# Ensure the following ops & var are assigned on GPU\n",
        "# (some ops are nmot compatible on GPU)\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  # Create the embedding varaible (each row represent a word embedding vector)\n",
        "  embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
        "  # Construct the varibales for the NCE loss\n",
        "  nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
        "  nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "def get_embedding(x):\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    # Lookup the corresponding embedding vectors for each sample in X\n",
        "    x_embed = tf.nn.embedding_lookup(embedding, x)\n",
        "    return x_embed\n",
        "\n",
        "def nce_loss(x_embed, y):\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.nce_loss(weights=nce_weights,\n",
        "                       biases=nce_biases,\n",
        "                       labels=y,\n",
        "                       inputs=x_embed,\n",
        "                       num_sampled=num_sampled,\n",
        "                       num_classes=vocabulary_size)\n",
        "    )\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(x_embed):\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    # Compute the cosine similarity between input data embedding and very embedding vectors\n",
        "    x_embed = tf.cast(x_embed, tf.float32)\n",
        "    x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
        "    embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims = True))\n",
        "    cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n",
        "\n",
        "    return cosine_sim_op\n",
        "  \n",
        "# Define the optimizer\n",
        "optimizers = tf.optimizers.SGD(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4Z1371Az5xu"
      },
      "outputs": [],
      "source": [
        "# Optimization process\n",
        "def run_optimization(x, y):\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    # Wrap computation inside a GradientTape for automatic differentiation\n",
        "    with tf.GradientTape() as g:\n",
        "      emb = get_embedding(x)\n",
        "      loss = nce_loss(emb, y)\n",
        "    \n",
        "    # Compute gradients\n",
        "    gradients = g.gradient(loss, [embedding, nce_weights, nce_biases])\n",
        "\n",
        "    # Update W and b following gradients\n",
        "    optimizers.apply_gradients(zip(gradients, [embedding, nce_weights, nce_biases]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrWKmKy60wd-",
        "outputId": "f3f63640-b718-47e8-a3a7-d5b170086e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps: 1, loss: 537.4056396484375\n",
            "Evaluation...\n",
            "five nearest neighbors practiced, distress, concerning, montpellier, chs, werke, shipboard, bun,\n",
            "of nearest neighbors cathedrals, technicality, incarnation, personhood, flutter, ceramic, planet, gygax,\n",
            "going nearest neighbors alhazred, refering, messaging, arranges, pizarro, shatila, overrun, deirdre,\n",
            "hardware nearest neighbors crowley, scantily, swearing, nanjing, nongovernmental, intruders, esso, cultivar,\n",
            "american nearest neighbors slashdot, presque, bowing, antlers, louise, depicts, horses, monopoly,\n",
            "britain nearest neighbors peasant, xxvii, symmetry, barton, bah, rainwater, espouses, modest,\n",
            "steps: 10000, loss: 146.65516662597656\n",
            "steps: 20000, loss: 40.09314727783203\n",
            "steps: 30000, loss: 76.89292907714844\n",
            "steps: 40000, loss: 40.19158935546875\n",
            "steps: 50000, loss: 49.35409164428711\n",
            "steps: 60000, loss: 20.357053756713867\n",
            "steps: 70000, loss: 28.436458587646484\n",
            "steps: 80000, loss: 33.27979278564453\n",
            "steps: 90000, loss: 35.885292053222656\n",
            "steps: 100000, loss: 21.366798400878906\n",
            "steps: 110000, loss: 20.692245483398438\n",
            "steps: 120000, loss: 14.0086669921875\n",
            "steps: 130000, loss: 22.397193908691406\n",
            "steps: 140000, loss: 11.217778205871582\n",
            "steps: 150000, loss: 19.711456298828125\n",
            "steps: 160000, loss: 14.547483444213867\n",
            "steps: 170000, loss: 13.896050453186035\n",
            "steps: 180000, loss: 12.74627685546875\n",
            "steps: 190000, loss: 10.678319931030273\n",
            "steps: 200000, loss: 13.40518569946289\n",
            "Evaluation...\n",
            "five nearest neighbors four, three, six, two, seven, eight, one, zero,\n",
            "of nearest neighbors and, with, is, the, a, which, for, or,\n",
            "going nearest neighbors believed, method, direction, forms, status, islamic, used, bc,\n",
            "hardware nearest neighbors languages, man, his, into, may, on, all, often,\n",
            "american nearest neighbors s, in, and, after, from, see, b,\n",
            "britain nearest neighbors an, after, its, people, her, called, while,\n",
            "steps: 210000, loss: 17.976856231689453\n",
            "steps: 220000, loss: 18.42262840270996\n",
            "steps: 230000, loss: 19.73633575439453\n",
            "steps: 240000, loss: 38.333526611328125\n",
            "steps: 250000, loss: 18.957420349121094\n",
            "steps: 260000, loss: 12.999024391174316\n",
            "steps: 270000, loss: 11.297327995300293\n",
            "steps: 280000, loss: 12.14807415008545\n",
            "steps: 290000, loss: 7.862030982971191\n",
            "steps: 300000, loss: 8.791414260864258\n",
            "steps: 310000, loss: 7.241896152496338\n",
            "steps: 320000, loss: 13.035911560058594\n",
            "steps: 330000, loss: 9.516728401184082\n",
            "steps: 340000, loss: 7.064711570739746\n",
            "steps: 350000, loss: 9.851400375366211\n",
            "steps: 360000, loss: 7.734336853027344\n",
            "steps: 370000, loss: 10.17764663696289\n",
            "steps: 380000, loss: 9.1737699508667\n",
            "steps: 390000, loss: 16.353031158447266\n",
            "steps: 400000, loss: 6.971485614776611\n",
            "Evaluation...\n",
            "five nearest neighbors four, three, six, seven, eight, two, one, nine,\n",
            "of nearest neighbors and, from, the, all, in, while, with, a,\n",
            "going nearest neighbors life, under, called, at, an, his, like, then,\n",
            "hardware nearest neighbors s, by, and, then, from, people, called, is,\n",
            "american nearest neighbors b, english, british, d, french, john, early,\n",
            "britain nearest neighbors into, the, s, then, history, over, under, in,\n",
            "steps: 410000, loss: 10.221267700195312\n",
            "steps: 420000, loss: 11.782526016235352\n",
            "steps: 430000, loss: 9.315775871276855\n",
            "steps: 440000, loss: 14.244842529296875\n",
            "steps: 450000, loss: 8.197366714477539\n",
            "steps: 460000, loss: 8.09130573272705\n",
            "steps: 470000, loss: 7.090710639953613\n",
            "steps: 480000, loss: 9.246536254882812\n",
            "steps: 490000, loss: 5.729461669921875\n",
            "steps: 500000, loss: 6.547972679138184\n",
            "steps: 510000, loss: 8.189648628234863\n",
            "steps: 520000, loss: 6.374476432800293\n",
            "steps: 530000, loss: 8.923614501953125\n",
            "steps: 540000, loss: 7.308620452880859\n",
            "steps: 550000, loss: 7.652060508728027\n",
            "steps: 560000, loss: 8.7913818359375\n",
            "steps: 570000, loss: 6.950133323669434\n",
            "steps: 580000, loss: 6.688950538635254\n"
          ]
        }
      ],
      "source": [
        "# Words for testing\n",
        "x_test = np.array([word2id[w] for w in eval_words])\n",
        "# Run training for the given number of steps\n",
        "for step in range(1, num_steps+1):\n",
        "  batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
        "  run_optimization(batch_x, batch_y)\n",
        "\n",
        "  if step % display_step == 0 or step == 1:\n",
        "    loss = nce_loss(get_embedding(batch_x), batch_y)\n",
        "    print(f\"steps: {step}, loss: {loss}\")\n",
        "\n",
        "    # Evaluation\n",
        "  if step % eval_step == 0 or step == 1:\n",
        "    print(\"Evaluation...\")\n",
        "    sim = evaluate(get_embedding(x_test)).numpy()\n",
        "    for i in range(len(eval_words)):\n",
        "      top_k = 8  # number of nearest neighbors.\n",
        "      nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "      log_str = f\"{eval_words[i]} nearest neighbors\"\n",
        "      for k in range(top_k):\n",
        "        try:\n",
        "          tmp = id2word[nearest[k]].decode()\n",
        "          log_str = '{0} {1},'.format(log_str, tmp)\n",
        "        except Exception as e:\n",
        "          # print(e.__repr__())\n",
        "          pass\n",
        "        else:\n",
        "          pass\n",
        "      print(log_str)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-uo_OiWqosQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}