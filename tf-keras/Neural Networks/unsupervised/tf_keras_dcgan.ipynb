{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz5HoRs3iRfI"
      },
      "source": [
        "# Deep Convolutional Generative Adversarial Network Example\n",
        "\n",
        "Build a deep convolutional generative adversarial network (DCGAN) to generate digit images from a noise distribution with Tensorflow v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Om_o1mjaS4"
      },
      "source": [
        "References:\n",
        "\n",
        "- Unsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, 2016.\n",
        "- Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio. Aistats 9, 249-256\n",
        "- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Sergey Ioffe, Christian Szegedy. 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eP1WzMxjfEZ",
        "outputId": "2c378f59-c181-42e6-96b9-99c677b0df90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.49.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKmegFIbj7Ap"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXJEVzB8kE2I"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset parameters\n",
        "num_features = 784 # data features (img shape: 28*28).\n",
        "\n",
        "# Training parameters.\n",
        "lr_generator = 0.0002\n",
        "lr_discriminator = 0.0002\n",
        "training_steps = 20000\n",
        "batch_size = 128\n",
        "display_step = 500\n",
        "\n",
        "# Network parameters.\n",
        "noise_dim = 100 # Noise data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzEKdALhk146",
        "outputId": "fd39aeec-1aa9-4c9b-f5dd-dbda713dac77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Prepare MNIST data.\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Convert to float32.\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "x_train, x_test = x_train / 255., x_test / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELY1KKfok5UQ"
      },
      "outputs": [],
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E0uNAS3k-54"
      },
      "outputs": [],
      "source": [
        "# Create TF Model\n",
        "class Generator(Model):\n",
        "  # Set layers\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.fc1 = layers.Dense(7 * 7 * 128)\n",
        "    self.bn1 = layers.BatchNormalization()\n",
        "    self.conv2tr1 = layers.Conv2DTranspose(64, 5, strides=2, padding=\"SAME\")\n",
        "    self.bn2 = layers.BatchNormalization()\n",
        "    self.conv2tr2 = layers.Conv2DTranspose(1, 5, strides=2, padding=\"SAME\")\n",
        "\n",
        "  # Set forward pass\n",
        "  def call(self, x, is_training=False):\n",
        "    x = self.fc1(x)\n",
        "    x = self.bn1(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "\n",
        "    # Reshape to a 4D array of images: (batch, height, width, channels)\n",
        "    # New shape: (batch, 7, 7, 128)\n",
        "    x = tf.reshape(x, shape=[-1, 7, 7, 128])\n",
        "    # Deconvulotion, image shape: (batch, 14, 14, 64)\n",
        "    x = self.conv2tr1(x)\n",
        "    x = self.bn2(x, training=is_training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    # Deconvolution, image shape (batch, 28, 28, 1)\n",
        "    x = self.conv2tr2(x)\n",
        "    x = tf.nn.tanh(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Generator Network\n",
        "# Input: Noise, Output: Image\n",
        "# Note that batch normalization has different behaviour at training and inference time\n",
        "# we then use a placeholder to indicate the layer if we are training or not\n",
        "\n",
        "class Discriminator(Model):\n",
        "  # Set layers\n",
        "  def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = layers.Conv2D(64, 5, strides=2, padding='SAME')\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.conv2 = layers.Conv2D(128, 5, strides=2, padding='SAME')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(1024)\n",
        "        self.bn3 = layers.BatchNormalization()\n",
        "        self.fc2 = layers.Dense(2)\n",
        "\n",
        "  # Set forward pass.\n",
        "  def call(self, x, is_training=False):\n",
        "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x, training=is_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=is_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn3(x, training=is_training)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "        return self.fc2(x)\n",
        "    \n",
        "\n",
        "# Build neural netowrk model\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_QlRz3jorKD"
      },
      "outputs": [],
      "source": [
        "# Losses\n",
        "def generator_loss(reconstructed_images) -> float:\n",
        "  gen_loss: float = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=reconstructed_images, labels=tf.ones([batch_size], dtype=tf.int32)\n",
        "  ))\n",
        "  return gen_loss\n",
        "\n",
        "def discriminator_loss(disc_fake, disc_real) -> float:\n",
        "  disc_loss_real: float = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=disc_real, labels=tf.ones([batch_size], dtype=tf.int32)\n",
        "  ))\n",
        "  disc_loss_fake: float = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      logits=disc_fake, labels=tf.ones([batch_size], dtype=tf.int32)\n",
        "  ))\n",
        "  \n",
        "  return disc_loss_real + disc_loss_fake\n",
        "\n",
        "# Optimizers\n",
        "optimizer_gen = tf.optimizers.Adam(learning_rate=lr_generator) # beta_1=0.5, beta_2=0.99\n",
        "optimizer_disc = tf.optimizers.Adam(learning_rate=lr_discriminator) # beta_1=0.5, beta_2=0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9sBhYkJqFUq"
      },
      "outputs": [],
      "source": [
        "# Optimization process. Inputs: real and noise\n",
        "def run_optimization(real_images):\n",
        "\n",
        "  # Rescale to [-1, 1], the input range of the discriminator\n",
        "  real_images = real_images * 2.0 - 1.0\n",
        "\n",
        "  # Generate Noise\n",
        "  noise = np.random.normal(-1.0, 1.0, size=[batch_size, noise_dim]).astype(np.float32)\n",
        "\n",
        "  with tf.GradientTape() as g:\n",
        "    fake_images = generator(noise, is_training=True)\n",
        "    disc_fake = discriminator(fake_images, is_training=True)\n",
        "    disc_real = discriminator(real_images, is_training=True)\n",
        "\n",
        "    disc_loss: float = discriminator_loss(disc_fake, disc_real)\n",
        "  \n",
        "  # Training Variables for each optimizer\n",
        "  gradients_disc = g.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  optimizer_disc.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
        "\n",
        "  # Generate Noise\n",
        "  noise = np.random.normal(-1.0, 1.0, size=[batch_size, noise_dim]).astype(np.float32)\n",
        "\n",
        "  with tf.GradientTape() as g:\n",
        "    fake_images = generator(noise, is_training=True)\n",
        "    disc_fake = discriminator(fake_images, is_training=True)\n",
        "\n",
        "    gen_loss = generator_loss(disc_fake)\n",
        "  \n",
        "  gradients_gen = g.gradient(gen_loss, generator.trainable_variables)\n",
        "  optimizer_gen.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
        "\n",
        "  return gen_loss, disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwRBVRmJsXa5",
        "outputId": "f407036a-96c8-4239-b4dd-c521486f7484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial: generator loss: 0.6894901990890503, discriminator loss: 1.3845398426055908\n",
            "Step: 500, generator loss: 0.0019445784855633974, discriminator loss: 0.0028123047668486834\n",
            "Step: 1000, generator loss: 0.0006655532633885741, discriminator loss: 0.0009768111631274223\n",
            "Step: 1500, generator loss: 0.0003221117949578911, discriminator loss: 0.0004940598737448454\n",
            "Step: 2000, generator loss: 0.0001908612612169236, discriminator loss: 0.0002851457102224231\n",
            "Step: 2500, generator loss: 0.00012523136683739722, discriminator loss: 0.00018676454783417284\n",
            "Step: 3000, generator loss: 8.692976552993059e-05, discriminator loss: 0.0001265893515665084\n",
            "Step: 3500, generator loss: 6.305928400252014e-05, discriminator loss: 9.020417201099917e-05\n",
            "Step: 4000, generator loss: 4.316943886806257e-05, discriminator loss: 6.424854655051604e-05\n",
            "Step: 4500, generator loss: 3.233210736652836e-05, discriminator loss: 4.7906396503094584e-05\n"
          ]
        }
      ],
      "source": [
        "# Run training for the given number of steps\n",
        "for step, (batch_x, _) in enumerate(train_data.take(training_steps + 1)):\n",
        "  if step == 0:\n",
        "    # Generate noise\n",
        "    noise = np.random.normal(-1.0, 1.0, size=[batch_size, noise_dim]).astype(np.float32)\n",
        "    gen_loss = generator_loss(discriminator(generator(noise)))\n",
        "    disc_loss = discriminator_loss(discriminator(batch_x), discriminator(generator(noise)))\n",
        "    print(f\"Initial: generator loss: {gen_loss}, discriminator loss: {disc_loss}\")\n",
        "    continue\n",
        "  \n",
        "  # Run the optimization\n",
        "  gen_loss, disc_loss = run_optimization(batch_x)\n",
        "\n",
        "  if step % display_step == 0:\n",
        "    print(f\"Step: {step}, generator loss: {gen_loss}, discriminator loss: {disc_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkPeNu3Zuz7h"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions.\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WLxOfXEou1Ug"
      },
      "outputs": [],
      "source": [
        "Testing\n",
        "# Generate images from noise, using the generator network.\n",
        "n = 6\n",
        "canvas = np.empty((28 * n, 28 * n))\n",
        "for i in range(n):\n",
        "    # Noise input.\n",
        "    z = np.random.normal(-1., 1., size=[n, noise_dim]).astype(np.float32)\n",
        "    # Generate image from noise.\n",
        "    g = generator(z).numpy()\n",
        "    # Rescale to original [0, 1]\n",
        "    g = (g + 1.) / 2\n",
        "    # Reverse colours for better display\n",
        "    g = -1 * (g - 1)\n",
        "    for j in range(n):\n",
        "        # Draw the generated digits\n",
        "        canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
        "\n",
        "plt.figure(figsize=(n, n))\n",
        "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}