{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJQXpT9o+C8rcod5Vsv2qC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Word2Vec (Word Embedding)\n","\n","Implement Word2Vec algorithm to compute vector representation of words, with Tensorflow 2.0. This example is using a small chunk of Wikipedia to train from.\n","\n","More info: Mikolov, Tomas et al. \"Efficient Estimation of Word Representations in Vector Space.\", 2013"],"metadata":{"id":"QXQESE1Np3nM"}},{"cell_type":"code","source":["! pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IO62urt9qgRd","executionInfo":{"status":"ok","timestamp":1665451839340,"user_tz":240,"elapsed":4640,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}},"outputId":"54364551-52d4-4523-84d9-f8f36fd65b57"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220929150707)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.49.1)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (22.9.24)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.1)\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import urllib\n","import zipfile\n","import collections\n","\n","import numpy as np\n","import tensorflow as tf"],"metadata":{"id":"OjGwgB4cqJJt","executionInfo":{"status":"ok","timestamp":1665460291641,"user_tz":240,"elapsed":287,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":136,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","\n","# Training Parameters\n","learning_rate: float = 0.1\n","batch_size: int = 128\n","num_steps: int = 3000000\n","display_step: int = 10000\n","eval_step: int = 200000\n","\n","# Evaluate Parameters\n","eval_words: List[str] = [\"five\", \"of\", \"going\", \"hardware\", \"american\", \"britain\"]\n","\n","# Word2Vec Parameters\n","embedding_size: int = 200 # Dimension of the embedding vector\n","max_vocabulary_size: int = 50000 # Total number of different words in the vocabulary\n","min_occurence: int = 10 # Remove all words that does not appear at least n times\n","skip_window: int = 3 # How many times to consider left and right\n","num_skips: int = 2 # How many times to reuse an input to generate a label\n","num_sampled: int = 64 # Number of negative example to sample"],"metadata":{"id":"TpsUXJJhqm-F","executionInfo":{"status":"ok","timestamp":1665460293347,"user_tz":240,"elapsed":2,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":137,"outputs":[]},{"cell_type":"code","source":["# Download a small chunk of Wikipedia articles collection.\n","url: str = 'http://mattmahoney.net/dc/text8.zip'\n","data_path = \"text8.zip\"\n","\n","test_words: str\n","\n","if not os.path.exists(data_path):\n","  print(\"Downloading the dataset... (It may take some time)\")\n","  filename, _ = urllib.request.urlretrieve(url, data_path)\n","  print(\"Done!\")\n","\n","# Unzip the dataset file. Text has already been processed\n","with zipfile.ZipFile(data_path) as f:\n","  text_words = f.read(f.namelist()[0]).lower().split()"],"metadata":{"id":"5H2ecqVhrtB2","executionInfo":{"status":"ok","timestamp":1665460296593,"user_tz":240,"elapsed":1817,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":138,"outputs":[]},{"cell_type":"code","source":["# Build the dictionary and replace rare words with UNK token\n","count = [(\"UNK\", -1)]\n","# Retrieve the most common words\n","count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n","# Remove samples with less than 'min_occurence' occurences\n","for i in range(len(count) -1, -1, -1):\n","  if count[i][1] < min_occurence:\n","    count.pop(i)\n","  else:\n","    # Retrieve a word if, or assign it index 0 (\"UNK\") if not in dictionary\n","    break\n","# Compute the vocabulary size.\n","vocabulary_size = len(count)\n","# Assign an id to each word\n","word2id = dict()\n","for i, (word, _) in enumerate(count):\n","  try:\n","    word2id[word.decode(\"utf8\")] = i\n","  except Exception as e:\n","    print(\"Not a byte\")\n","  else:\n","    word2id[word] = i\n","\n","data = list()\n","unk_count = 0\n","\n","for word in text_words:\n","  # Retrieve a word id or assign it index 0 ('UNK') if not in dictionary\n","  index = word2id.get(word, 0)\n","  if index == 0:\n","    unk_count = unk_count + 1\n","  data.append(index)\n","\n","count[0] = (\"UNK\", unk_count)\n","id2word = dict(zip(word2id.values(), word2id.keys()))\n","\n","print(\"Words count:\", len(text_words))\n","print(\"Unique words:\", len(set(text_words)))\n","print(\"Vocabulary size:\", vocabulary_size)\n","print(\"Most common words:\", count[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRscIOxQsu5e","executionInfo":{"status":"ok","timestamp":1665460308621,"user_tz":240,"elapsed":9159,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}},"outputId":"acb0a81a-037b-4b78-8373-82665c79b2cc"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Not a byte\n","Words count: 17005207\n","Unique words: 253854\n","Vocabulary size: 47135\n","Most common words: [('UNK', 444176), (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764), (b'in', 372201), (b'a', 325873), (b'to', 316376), (b'zero', 264975), (b'nine', 250430)]\n"]}]},{"cell_type":"code","source":["data_index: int = 0\n","# Generate training batch for the skip-gram model\n","def next_batch(batch_size: int, num_skips: int, skip_window: int):\n","  global data_index\n","  assert batch_size % num_skips == 0\n","  assert num_skips <= 2 * skip_window\n","\n","  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n","  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n","  # get window size (words left and right and current one)\n","  span = 2 * skip_window + 1\n","  buffer = collections.deque(maxlen=span)\n","  if data_index + span > len(data):\n","    data_index = 0\n","  buffer.extend(data[data_index:data_index + span])\n","  data_index = data_index + span\n","\n","  for i in range(batch_size // num_skips):\n","    context_words = [w for w in range(span) if w != skip_window]\n","    words_to_use = random.sample(context_words, num_skips)\n","    for j, context_word in enumerate(words_to_use):\n","      batch[i * num_skips + j] = buffer[skip_window]\n","      labels[i * num_skips + j, 0] = buffer[context_word]\n","\n","      if data_index == len(data):\n","        buffer.extend(data[0:span])\n","        data_index = span\n","      else:\n","        buffer.append(data[data_index])\n","        data_index = data_index + 1\n","  # Backtrack a little bit to avoid skipping words in the end of a batch\n","  data_index = (data_index + len(data) - span) % len(data)\n","  return batch, labels"],"metadata":{"id":"NUbMMpEhu_Dm","executionInfo":{"status":"ok","timestamp":1665460313354,"user_tz":240,"elapsed":282,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":140,"outputs":[]},{"cell_type":"code","source":["# Ensure the following ops & var are assigned on GPU\n","# (some ops are nmot compatible on GPU)\n","\n","with tf.device(\"/cpu:0\"):\n","  # Create the embedding varaible (each row represent a word embedding vector)\n","  embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n","  # Construct the varibales for the NCE loss\n","  nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n","  nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n","\n","def get_embedding(x):\n","  with tf.device(\"/cpu:0\"):\n","    # Lookup the corresponding embedding vectors for each sample in X\n","    x_embed = tf.nn.embedding_lookup(embedding, x)\n","    return x_embed\n","\n","def nce_loss(x_embed, y):\n","  with tf.device(\"/cpu:0\"):\n","    y = tf.cast(y, tf.int64)\n","    loss = tf.reduce_mean(\n","        tf.nn.nce_loss(weights=nce_weights,\n","                       biases=nce_biases,\n","                       labels=y,\n","                       inputs=x_embed,\n","                       num_sampled=num_sampled,\n","                       num_classes=vocabulary_size)\n","    )\n","\n","    return loss\n","\n","# Evaluation\n","def evaluate(x_embed):\n","  with tf.device(\"/cpu:0\"):\n","    # Compute the cosine similarity between input data embedding and very embedding vectors\n","    x_embed = tf.cast(x_embed, tf.float32)\n","    x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n","    embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims = True))\n","    cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n","\n","    return cosine_sim_op\n","  \n","# Define the optimizer\n","optimizers = tf.optimizers.SGD(learning_rate)"],"metadata":{"id":"A4NVbWk0xalV","executionInfo":{"status":"ok","timestamp":1665460324132,"user_tz":240,"elapsed":830,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":141,"outputs":[]},{"cell_type":"code","source":["# Optimization process\n","def run_optimization(x, y):\n","  with tf.device(\"/cpu:0\"):\n","    # Wrap computation inside a GradientTape for automatic differentiation\n","    with tf.GradientTape() as g:\n","      emb = get_embedding(x)\n","      loss = nce_loss(emb, y)\n","    \n","    # Compute gradients\n","    gradients = g.gradient(loss, [embedding, nce_weights, nce_biases])\n","\n","    # Update W and b following gradients\n","    optimizers.apply_gradients(zip(gradients, [embedding, nce_weights, nce_biases]))"],"metadata":{"id":"T4Z1371Az5xu","executionInfo":{"status":"ok","timestamp":1665460329094,"user_tz":240,"elapsed":236,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}}},"execution_count":142,"outputs":[]},{"cell_type":"code","source":["# Words for testing\n","x_test = np.array([word2id[w] for w in eval_words])\n","# Run training for the given number of steps\n","for step in range(1, num_steps+1):\n","  batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n","  run_optimization(batch_x, batch_y)\n","\n","  if step % display_step == 0 or step == 1:\n","    loss = nce_loss(get_embedding(batch_x), batch_y)\n","    print(f\"steps: {step}, loss: {loss}\")\n","\n","    # Evaluation\n","  if step % display_step == 0 or step == 1:\n","    print(\"Evaluation...\")\n","    sim = evaluate(get_embedding(x_test)).numpy()\n","    for i in range(len(eval_words)):\n","      top_k = 8 # number of nearest neighbours\n","      nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n","      log_str = f\"{eval_words[i]} nearest neighbours\"\n","      for k in range(top_k):\n","        log_str = f\"{log_str} {id2word[nearest[k]]}\"\n","      print(log_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xrWKmKy60wd-","executionInfo":{"status":"error","timestamp":1665461711722,"user_tz":240,"elapsed":1186137,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}},"outputId":"b40f9234-e899-4389-b348-545e1420cd81"},"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["steps: 1, loss: 520.2025756835938\n","Evaluation...\n","five nearest neighbours b'hide' b'gurion' b'observe' b'bayern' b'sura' b'unlikely' b'puzzles' b'fairs'\n","of nearest neighbours b'hillbillies' b'collectors' b'blowfish' b'gul' b'element' b'consuls' b'scuderia' b'punch'\n","going nearest neighbours b'khartoum' b'mighty' b'lesbians' b'fifth' b'owl' b'whistler' b'hmso' b'undiscovered'\n","hardware nearest neighbours b'ruth' b'oblast' b'publications' b'intent' b'etiology' b'reprint' b'violating' b'dll'\n","american nearest neighbours b'hail' b'veterinarian' b'dialogue' b'medicinal' b'spitzer' b'estrada' b'gaeilge' b'renarrative'\n","britain nearest neighbours b'ayn' b'chan' b'iie' b'terracotta' b'machinations' b'knee' b'paley' b'marquette'\n","steps: 10000, loss: 142.07131958007812\n","Evaluation...\n","five nearest neighbours b'three' b'six' b'eight' b'four' b'two' b'seven' b'one' b'nine'\n","of nearest neighbours b'the' b'in' b'and' b'a' b'to' b'by' b'for' b'is'\n","going nearest neighbours b'khartoum' b'fifth' b'reinforced' b'mighty' b'undiscovered' b'abnormal' b'bakunin' b'amusing'\n","hardware nearest neighbours b'oblast' b'ruth' b'intent' b'publications' b'etiology' b'yap' b'dll' b'radio'\n","american nearest neighbours b'six' b'have' b'an' b'four' b'five' b'nine' b'it' b'three'\n","britain nearest neighbours b'very' b'into' b'between' b'chan' b'new' b'raised' b'ayn' b'people'\n","steps: 20000, loss: 87.02629089355469\n","Evaluation...\n","five nearest neighbours b'four' b'six' b'three' b'eight' b'seven' b'two' b'one' b'with'\n","of nearest neighbours b'and' b'in' b'the' b'a' b'to' b'with' b'as' b'from'\n","going nearest neighbours b'references' b'information' b'times' b'last' b'fifth' b'series' b'forms' b'same'\n","hardware nearest neighbours b'radio' b'under' b'links' b'list' b'do' b'alexander' b'than' b'since'\n","american nearest neighbours b'also' b'from' b'have' b'an' b'their' b'has' b'not' b'or'\n","britain nearest neighbours b'into' b'between' b'very' b'new' b'over' b'people' b'like' b'no'\n","steps: 30000, loss: 71.87934112548828\n","Evaluation...\n","five nearest neighbours b'three' b'four' b'two' b'six' b'one' b'eight' b'nine' b'seven'\n","of nearest neighbours b'and' b'the' b'is' b'in' b'to' b'a' b'from' b'with'\n","going nearest neighbours b'series' b'times' b'same' b'last' b'information' b'however' b'references' b'found'\n","hardware nearest neighbours b'ruth' b'radio' b'links' b'under' b'list' b'do' b'point' b'baseball'\n","american nearest neighbours b'also' b'most' b'which' b'an' b'by' b'they' b'on' b'their'\n","britain nearest neighbours b'between' b'very' b'into' b'over' b'people' b'like' b'new' b'time'\n","steps: 40000, loss: 45.79440689086914\n","Evaluation...\n","five nearest neighbours b'six' b'three' b'two' b'four' b'eight' b'one' b'seven' b'nine'\n","of nearest neighbours b'and' b'a' b'to' b'in' b'for' b'with' b'or' b'the'\n","going nearest neighbours b'series' b'times' b'information' b'same' b'last' b'found' b'references' b'popular'\n","hardware nearest neighbours b'under' b'ruth' b'radio' b'links' b'do' b'list' b'law' b'through'\n","american nearest neighbours b'also' b'most' b'many' b'from' b'first' b'their' b'or' b'who'\n","britain nearest neighbours b'very' b'between' b'over' b'people' b'like' b'into' b'time' b'states'\n","steps: 50000, loss: 44.301265716552734\n","Evaluation...\n","five nearest neighbours b'four' b'three' b'two' b'eight' b'seven' b'zero' b'six' b'one'\n","of nearest neighbours b'the' b'and' b'a' b'in' b'as' b'to' b'is' b'with'\n","going nearest neighbours b'times' b'series' b'same' b'last' b'information' b'found' b'references' b'popular'\n","hardware nearest neighbours b'under' b'links' b'radio' b'list' b'do' b'law' b'point' b'th'\n","american nearest neighbours b'from' b'their' b'also' b'or' b'with' b'by' b'as' b'has'\n","britain nearest neighbours b'very' b'between' b'over' b'like' b'found' b'people' b'war' b'states'\n","steps: 60000, loss: 25.115230560302734\n","Evaluation...\n","five nearest neighbours b'four' b'three' b'six' b'two' b'eight' b'seven' b'nine' b'one'\n","of nearest neighbours b'in' b'a' b'and' b'to' b'the' b'is' b'with' b'for'\n","going nearest neighbours b'times' b'last' b'series' b'information' b'same' b'found' b'similar' b'references'\n","hardware nearest neighbours b'under' b'radio' b'links' b'law' b'do' b'list' b'point' b'th'\n","american nearest neighbours b'some' b'as' b'first' b'also' b'are' b'or' b'its' b'from'\n","britain nearest neighbours b'very' b'found' b'like' b'between' b'south' b'over' b'g' b'people'\n","steps: 70000, loss: 39.746070861816406\n","Evaluation...\n","five nearest neighbours b'three' b'four' b'six' b'seven' b'two' b'eight' b'one' b'nine'\n","of nearest neighbours b'and' b'a' b'in' b'the' b'to' b'by' b'which' b'an'\n","going nearest neighbours b'times' b'last' b'information' b'series' b'found' b'similar' b'same' b'references'\n","hardware nearest neighbours b'radio' b'under' b'links' b'law' b'point' b'do' b'list' b'term'\n","american nearest neighbours b's' b'from' b'and' b'with' b'which' b'on' b'as' b'first'\n","britain nearest neighbours b'very' b'found' b'like' b'south' b'g' b'between' b'german' b'states'\n","steps: 80000, loss: 37.237056732177734\n","Evaluation...\n","five nearest neighbours b'six' b'four' b'three' b'seven' b'eight' b'two' b'nine' b'one'\n","of nearest neighbours b'and' b'the' b'to' b'a' b'in' b'but' b'is' b'as'\n","going nearest neighbours b'times' b'last' b'similar' b'forms' b'found' b'references' b'series' b'information'\n","hardware nearest neighbours b'radio' b'law' b'point' b'links' b'under' b'list' b'society' b'do'\n","american nearest neighbours b'on' b'in' b'by' b'from' b's' b'was' b'an' b'and'\n","britain nearest neighbours b'very' b'found' b'south' b'like' b'g' b'german' b'north' b'modern'\n","steps: 90000, loss: 32.41535186767578\n","Evaluation...\n","five nearest neighbours b'three' b'four' b'six' b'two' b'seven' b'eight' b'one' b'nine'\n","of nearest neighbours b'and' b'by' b'a' b'to' b'which' b'for' b'also' b'the'\n","going nearest neighbours b'times' b'last' b'forms' b'similar' b'references' b'area' b'found' b'information'\n","hardware nearest neighbours b'radio' b'point' b'law' b'main' b'society' b'third' b'ruth' b'links'\n","american nearest neighbours b'an' b'with' b'for' b'on' b'and' b'by' b'first' b'or'\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-147-55ddc16f30bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mlog_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{eval_words[i]} nearest neighbours\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlog_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{log_str} {id2word[nearest[k]]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 0"]}]},{"cell_type":"code","source":["print(id2word[nearest[0]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xpa_9xReLYL1","executionInfo":{"status":"ok","timestamp":1665460497762,"user_tz":240,"elapsed":228,"user":{"displayName":"AD Wambo","userId":"05080342455693343294"}},"outputId":"e0a62c20-13aa-48e7-c428-3d21e9df6066"},"execution_count":146,"outputs":[{"output_type":"stream","name":"stdout","text":["b'were'\n"]}]}]}